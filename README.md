# Tech News Aggregator

An automated tech news aggregator that scrapes multiple sources daily, processes content with AI embeddings for deduplication, and generates PDF digests.

## Features

- ğŸ”„ **Automated Scraping**: RSS feed parsing and web scraping with Crawl4AI
- ğŸ¤– **AI-Powered Deduplication**: Uses OpenAI embeddings to identify and remove duplicate articles
- ğŸ—„ï¸ **Vector Storage**: Stores articles with embeddings in Supabase with pgvector
- ğŸ“„ **PDF Generation**: Creates beautifully formatted PDF digests using WeasyPrint
- ğŸš€ **FastAPI**: High-performance async API for integration
- ğŸ”— **n8n Integration**: Ready-to-use workflows for automation

## Tech Stack

- **Orchestration**: n8n (self-hosted)
- **Scraping**: Python 3.11+ with Crawl4AI, Feedparser, Requests
- **API**: FastAPI with async support
- **Embeddings**: OpenAI text-embedding-ada-002
- **Vector Store**: Supabase with pgvector extension
- **Output**: Markdown to PDF (WeasyPrint)

## Quick Start

### Prerequisites

- Python 3.11 or higher
- OpenAI API key
- Supabase account (optional, for storage)
- n8n instance (optional, for automation)

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/ArturoAmador47/web-screpper.git
cd web-screpper
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Configure environment variables:**
```bash
cp .env.example .env
# Edit .env with your configuration
```

Required environment variables:
```env
OPENAI_API_KEY=your_openai_api_key_here
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_key_here
NEWS_SOURCES=https://techcrunch.com/feed/,https://www.theverge.com/rss/index.xml
```

4. **Set up Supabase (optional):**

Run the SQL from `src/storage/supabase_storage.py` in your Supabase SQL editor:
```bash
python -m src.storage.supabase_storage
```

This will print the SQL needed to create the articles table and vector search function.

### Usage

#### Run the API Server

```bash
python -m src.api.main
```

The API will be available at `http://localhost:8000`

API Documentation: `http://localhost:8000/docs`

#### Run the Aggregator Directly

```bash
python -m src.aggregator
```

#### API Endpoints

- `POST /scrape` - Trigger news scraping and processing
- `GET /articles` - Retrieve stored articles
- `GET /pdfs` - List generated PDFs
- `GET /pdf/{filename}` - Download a specific PDF
- `POST /webhook/n8n` - n8n webhook endpoint

Example API call:
```bash
curl -X POST http://localhost:8000/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "deduplicate": true,
    "store": true,
    "generate_pdf": true
  }'
```

#### n8n Integration

Import the workflows from `n8n_workflows/`:

1. **Daily Aggregator**: Automated daily news digest with email delivery
2. **Webhook Trigger**: On-demand scraping via webhook

See [n8n_workflows/README.md](n8n_workflows/README.md) for detailed setup instructions.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   n8n       â”‚  Orchestration & Scheduling
â”‚  Workflows  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Server                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         News Aggregator                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚  â”‚  Scraper  â”‚  â”‚  Embeddings  â”‚        â”‚   â”‚
â”‚  â”‚  â”‚ (Crawl4AI)â”‚  â”‚   (OpenAI)   â”‚        â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â”‚        â”‚                â”‚                â”‚   â”‚
â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚   â”‚
â”‚  â”‚                 â–¼                        â”‚   â”‚
â”‚  â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚        â”‚  Deduplication  â”‚              â”‚   â”‚
â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â”‚                 â”‚                        â”‚   â”‚
â”‚  â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚        â–¼                 â–¼              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚  â”‚ Supabase â”‚    â”‚     PDF      â”‚      â”‚   â”‚
â”‚  â”‚  â”‚ (pgvector)â”‚    â”‚  Generator   â”‚      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
web-screpper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â””â”€â”€ news_scraper.py      # RSS and web scraping
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ embeddings_service.py # OpenAI embeddings
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ supabase_storage.py  # Database operations
â”‚   â”œâ”€â”€ pdf_generator/
â”‚   â”‚   â””â”€â”€ pdf_service.py       # PDF generation
â”‚   â”œâ”€â”€ aggregator.py            # Main orchestration
â”‚   â””â”€â”€ config.py                # Configuration management
â”œâ”€â”€ n8n_workflows/
â”‚   â”œâ”€â”€ daily_news_aggregator.json
â”‚   â”œâ”€â”€ webhook_trigger.json
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ tests/                       # Test files
â”œâ”€â”€ output/                      # Generated PDFs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## Configuration

All configuration is managed through environment variables in `.env`:

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key | Required |
| `SUPABASE_URL` | Supabase project URL | Required |
| `SUPABASE_KEY` | Supabase API key | Required |
| `NEWS_SOURCES` | Comma-separated RSS feed URLs | "" |
| `EMBEDDING_MODEL` | OpenAI embedding model | text-embedding-ada-002 |
| `SIMILARITY_THRESHOLD` | Duplicate detection threshold | 0.85 |
| `API_HOST` | API server host | 0.0.0.0 |
| `API_PORT` | API server port | 8000 |
| `OUTPUT_DIR` | PDF output directory | ./output |

## How It Works

1. **Scraping**: The scraper fetches articles from configured RSS feeds using Feedparser. For full article content, it can use Crawl4AI for intelligent web scraping.

2. **Embedding Generation**: Each article's title and content are combined and sent to OpenAI's embedding API to generate a 1536-dimensional vector representation.

3. **Deduplication**: Articles are compared using cosine similarity of their embeddings. Articles above the similarity threshold are considered duplicates, and only one is kept.

4. **Storage**: Unique articles and their embeddings are stored in Supabase with pgvector, enabling fast similarity searches.

5. **PDF Generation**: Articles are formatted into a styled PDF document using WeasyPrint, with markdown rendering for rich formatting.

6. **Automation**: n8n workflows can trigger the pipeline on a schedule or via webhooks, with results delivered via email or other channels.

## Development

### Running Tests

```bash
pytest tests/
```

### Code Style

The project follows PEP 8 style guidelines. Format code with:
```bash
black src/
```

## API Documentation

Interactive API documentation is available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## Troubleshooting

### Common Issues

1. **ImportError for Crawl4AI**: Crawl4AI is optional. If not installed, the scraper falls back to using requests for web scraping.

2. **Supabase connection errors**: Ensure your Supabase URL and key are correct and the database schema is set up.

3. **OpenAI rate limits**: The embeddings service processes articles in batches to avoid rate limits. Adjust batch size if needed.

4. **WeasyPrint dependencies**: WeasyPrint requires system libraries. On Linux:
   ```bash
   sudo apt-get install python3-dev python3-pip python3-cffi libcairo2 libpango-1.0-0 libpangocairo-1.0-0
   ```

## Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

MIT License - see LICENSE file for details

## Roadmap

- [ ] Support for more news sources and formats
- [ ] Advanced deduplication using semantic analysis
- [ ] Multi-language support
- [ ] Custom PDF templates
- [ ] Article categorization and tagging
- [ ] Real-time streaming API
- [ ] Web dashboard for monitoring
- [ ] Docker containerization

## Support

For issues and questions:
- GitHub Issues: [Create an issue](https://github.com/ArturoAmador47/web-screpper/issues)
- Documentation: See inline code documentation

---

Built with â¤ï¸ using Python, FastAPI, and modern AI tools.
